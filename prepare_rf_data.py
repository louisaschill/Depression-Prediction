import pandas as pd
from sklearn.model_selection import train_test_split
import numpy as np
from pathlib import Path
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import joblib
import json

# DEPRESSION_THRESHOLD will be dynamically calculated, so the global constant is no longer primary.
# We can leave it commented out or remove if not needed as a fallback.
# DEPRESSION_THRESHOLD = 5 # Placeholder - PLEASE UPDATE

def load_processed_data(file_path='results/merged_variables.csv'):
    """Loads the preprocessed data."""
    data_path = Path(file_path)
    if not data_path.exists():
        # Here, you might want to add a call to run the main() function of merge_all_variables.py
        # For now, we'll raise an error if the file doesn't exist.
        raise FileNotFoundError(
            f"Processed data file not found: {file_path}. "
            "Please run merge_all_variables.py first or ensure the path is correct."
        )
    return pd.read_csv(data_path)

def define_features_target(df, target_column='3_yr_depress_score', binarize_target=True, percentile_threshold=None, fixed_threshold=None):
    """
    Defines feature matrix (X) and target vector (y).
    Optionally binarizes the target variable using a percentile or a fixed threshold.
    """
    # Drop rows where the target column is NaN, if any (should be handled by get_reference_cohort mostly)
    df_cleaned = df.dropna(subset=[target_column]).copy()

    if binarize_target:
        current_threshold = None
        if percentile_threshold is not None:
            current_threshold = df_cleaned[target_column].quantile(percentile_threshold)
            print(f"Target variable '{target_column}' binarized using {percentile_threshold*100:.0f}th percentile threshold > {current_threshold:.2f}.")
        elif fixed_threshold is not None:
            current_threshold = fixed_threshold
            print(f"Target variable '{target_column}' binarized using fixed threshold > {current_threshold:.2f}.")
        else:
            # Default to median if no threshold specified for binarization
            current_threshold = df_cleaned[target_column].median()
            print(f"Warning: No threshold specified for binarization. Defaulting to median threshold > {current_threshold:.2f}.")

        df_cleaned['target_binary'] = (df_cleaned[target_column] > current_threshold).astype(int)
        y = df_cleaned['target_binary']
        print(f"Class distribution:\n{y.value_counts(normalize=True)}")
    else:
        y = df_cleaned[target_column]
        print(f"Using continuous target variable '{target_column}'.")

    X = df_cleaned.drop(columns=['src_subject_id', target_column] + (['target_binary'] if binarize_target else []))
    
    # Ensure no NaN values remain in X after preprocessing steps in merge_all_variables.py
    # If they do, a strategy (e.g., re-imputation or row removal) would be needed here.
    # For now, we assume merge_all_variables.py handles imputation thoroughly.
    # print(f"Checking for NaNs in X: {X.isnull().sum().sum()} NaNs found.")


    return X, y

def main():
    """Main function to prepare data for Random Forest."""
    print("Starting data preparation for Random Forest model...")

    # 1. Load processed data
    # Assumes 'results/merged_variables.csv' is generated by merge_all_variables.py
    try:
        processed_df = load_processed_data()
        print(f"Successfully loaded {len(processed_df)} rows from 'results/merged_variables.csv'.")
    except FileNotFoundError as e:
        print(e)
        return

    # 2. Define Features (X) and Target (y)
    # Binarizing using the 75th percentile as requested.
    percentile_to_use = 0.75
    X, y = define_features_target(processed_df, binarize_target=True, percentile_threshold=percentile_to_use)
    print(f"Features (X) shape: {X.shape}")
    print(f"Target (y) shape: {y.shape}")

    if X.empty or y.empty:
        print("X or y is empty. Cannot proceed with data splitting. Check data loading and processing.")
        return
        
    # 3. Split Data
    # Using a fixed random_state for reproducibility
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y if y.nunique() > 1 else None)

    print("\nData Splitting Complete:")
    print(f"X_train shape: {X_train.shape}")
    print(f"X_test shape: {X_test.shape}")
    print(f"y_train shape: {y_train.shape}")
    print(f"y_test shape: {y_test.shape}")

    # 4. Output: Briefly summarize and list first 5 columns of X_train
    print("\nFirst 5 columns of X_train:")
    print(X_train.iloc[:, :5].head())
    
    # Further check for NaNs introduced or persisting unexpectedly
    print(f"\nNaNs in X_train: {X_train.isnull().sum().sum()}")
    print(f"NaNs in X_test: {X_test.isnull().sum().sum()}")
    print(f"NaNs in y_train: {y_train.isnull().sum().sum()}")
    print(f"NaNs in y_test: {y_test.isnull().sum().sum()}")

    # --- Start of Prompt 2 Additions ---
    print("\n--- Starting Random Forest Model Training and Evaluation ---")

    # 1. Train Model
    print("Training Random Forest Classifier...")
    # Start with default hyperparameters, set random_state for reproducibility
    rf_classifier = RandomForestClassifier(random_state=42)
    rf_classifier.fit(X_train, y_train)
    print("Model training complete.")

    # 2. Evaluate Model
    print("\nEvaluating model on the test set...")
    y_pred = rf_classifier.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred) # Default is for class 1
    recall = recall_score(y_test, y_pred)       # Default is for class 1
    f1 = f1_score(y_test, y_pred)               # Default is for class 1
    cm = confusion_matrix(y_test, y_pred)

    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision (for class 1): {precision:.4f}")
    print(f"Recall (for class 1): {recall:.4f}")
    print(f"F1-score (for class 1): {f1:.4f}")
    print("Confusion Matrix:")
    print(cm)

    # 3. Feature Importances
    print("\nExtracting feature importances...")
    importances = rf_classifier.feature_importances_
    feature_names = X.columns
    feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})
    feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)

    print("\nTop 20 most important features:")
    print(feature_importance_df.head(20).to_string())

    # 4. Save Model (Recommended)
    model_filename = 'results/random_forest_model.joblib'
    print(f"\nSaving trained model to {model_filename}...")
    joblib.dump(rf_classifier, model_filename)
    print("Model saved.")

    # Save the feature names (X.columns)
    feature_names_filename = 'results/model_feature_names.json'
    print(f"\nSaving feature names to {feature_names_filename}...")
    with open(feature_names_filename, 'w') as f:
        json.dump(X.columns.tolist(), f)
    print("Feature names saved.")
    
    print("\n--- Random Forest Model Training and Evaluation Complete ---")
    # --- End of Prompt 2 Additions ---

if __name__ == "__main__":
    main() 