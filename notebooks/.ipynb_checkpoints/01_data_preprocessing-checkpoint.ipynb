{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de153cfc-ca5b-4fcb-9208-27583f938929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from scipy import stats\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('default') \n",
    "sns.set_theme()\n",
    "\n",
    "def load_cbcl_depression():\n",
    "    \"\"\"Load depression scores and clean\"\"\"\n",
    "    # Load CBCL data\n",
    "    cbcl_file = \"../data/core/mental-health/mh_p_cbcl.csv\"\n",
    "    cbcl_df = pd.read_csv(cbcl_file, low_memory=False)\n",
    "    \n",
    "    # Get 3-year follow-up \n",
    "    followup_mask = cbcl_df['eventname'].str.contains('3_year', case=False, na=False)\n",
    "    followup_df = cbcl_df[followup_mask].copy()\n",
    "    \n",
    "    # Get depression score \n",
    "    dep_col = [col for col in followup_df.columns if 'depress' in col.lower() and 'dsm5' in col.lower()][0]\n",
    "    \n",
    "    # Remove subjects with missing /r invalid (555,999,777) scores\n",
    "    invalid_scores = [555, 999, 777, np.nan]\n",
    "    valid_mask = ~followup_df[dep_col].isin(invalid_scores)\n",
    "    clean_df = followup_df[valid_mask].copy()\n",
    "    \n",
    "    # only get necessary columns\n",
    "    result_df = clean_df[['src_subject_id', dep_col]].copy()\n",
    "    result_df = result_df.rename(columns={dep_col: 'depression_score'})\n",
    "    \n",
    "    print(f\"Original subjects: {len(followup_df)}\")\n",
    "    print(f\"Subjects after cleaning: {len(result_df)}\")\n",
    "    print(f\"\\nDepression score statistics:\")\n",
    "    print(result_df['depression_score'].describe())\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def load_baseline_data(subject_ids):\n",
    "    \"\"\"Load baseline data for only  subjects with CBCL dsm5 depress scores. Get all data from all core subfolders, excluding CBCL scores\"\"\"\n",
    "    core_dir = Path(\"../data/core\")\n",
    "    all_data = []\n",
    "    \n",
    "    # Calculate min required subjects (80% of full cohort -- we get rid of files missing over 20%)\n",
    "    min_subjects = len(subject_ids) * 0.8\n",
    "    print(f\"Minimum required subjects per file: {min_subjects:.0f}\")\n",
    "    \n",
    "    # Process each subdirectory \n",
    "    for subdir in core_dir.iterdir():\n",
    "        if not subdir.is_dir():\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nProcessing {subdir.name}...\")\n",
    "        subdir_data = []\n",
    "        \n",
    "        # Skip CBCL directory\n",
    "        if 'cbcl' in subdir.name.lower():\n",
    "            print(f\"Skipping {subdir.name} (CBCL data excluded)\")\n",
    "            continue\n",
    "        \n",
    "        for file in subdir.glob('*.csv'):\n",
    "            try:\n",
    "                # don't get CBCL --- too colinear with future CBCL depression \n",
    "                if 'cbcl' in file.name.lower():\n",
    "                    print(f\"  - Skipping {file.name} (CBCL data excluded)\")\n",
    "                    continue\n",
    "                \n",
    "                # Load data\n",
    "                df = pd.read_csv(file, low_memory=False)\n",
    "                \n",
    "                # Check if file has subject ID column -- fixes issue \n",
    "                if 'src_subject_id' not in df.columns:\n",
    "                    continue\n",
    "                    \n",
    "                # Get baseline data\n",
    "                if 'eventname' in df.columns:\n",
    "                    baseline_mask = df['eventname'].str.contains('baseline', case=False, na=False)\n",
    "                    df = df[baseline_mask].copy()\n",
    "                \n",
    "                # Filter for our subjects\n",
    "                df = df[df['src_subject_id'].isin(subject_ids)].copy()\n",
    "                \n",
    "                # Only keep files with >80% subjects\n",
    "                if len(df) >= min_subjects:\n",
    "                    print(f\"  - {file.name}: {len(df)} subjects (keeping)\")\n",
    "                    # Add source information to column names\n",
    "                    df.columns = [f\"{col}_{subdir.name}_{file.stem}\" if col not in ['src_subject_id', 'eventname'] else col \n",
    "                                for col in df.columns]\n",
    "                    subdir_data.append(df)\n",
    "                else:\n",
    "                    print(f\"  - {file.name}: {len(df)} subjects (skipping - too few subjects)\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  - Error loading {file.name}: {str(e)}\")\n",
    "        \n",
    "        # Merge data within each subdirectory -- otherwise stalls out \n",
    "        if subdir_data:\n",
    "            print(f\"Merging {len(subdir_data)} files from {subdir.name}...\")\n",
    "            subdir_merged = subdir_data[0]\n",
    "            for i, df in enumerate(subdir_data[1:], 1):\n",
    "                suffixes = (f'_{i}', f'_{i+1}')\n",
    "                subdir_merged = pd.merge(subdir_merged, df, on='src_subject_id', how='outer', suffixes=suffixes)\n",
    "            all_data.append(subdir_merged)\n",
    "            print(f\"Completed {subdir.name} merge. Shape: {subdir_merged.shape}\")\n",
    "    \n",
    "    # Merge subdirectory dataframes\n",
    "    if all_data:\n",
    "        print(\"\\nMerging all subdirectories...\")\n",
    "        final_df = all_data[0]\n",
    "        for i, df in enumerate(all_data[1:], 1):\n",
    "            print(f\"Merging subdirectory {i+1} of {len(all_data)}...\")\n",
    "            suffixes = (f'_dir{i}', f'_dir{i+1}')\n",
    "            final_df = pd.merge(final_df, df, on='src_subject_id', how='outer', suffixes=suffixes)\n",
    "            print(f\"Current shape: {final_df.shape}\")\n",
    "        return final_df\n",
    "    return None\n",
    "\n",
    "def create_depression_categories(df):\n",
    "    \"\"\"Create depression categories using statistical methods to identify high scores\"\"\"\n",
    "    # Calculate statistics\n",
    "    mean = df['depression_score'].mean()\n",
    "    std = df['depression_score'].std()\n",
    "    q1 = df['depression_score'].quantile(0.25)\n",
    "    q3 = df['depression_score'].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    \n",
    "    # Calculate thresholds using both methods\n",
    "    z_score_threshold = mean + 2*std  # Scores > 2 SD above mean\n",
    "    iqr_threshold = q3 + 1.5*iqr      # Scores > Q3 + 1.5*IQR\n",
    "    \n",
    "    # Use IQR method (more robust to non-normal distribution)\n",
    "    high_threshold = iqr_threshold\n",
    "    \n",
    "    # Create categories\n",
    "    df['depression_category'] = pd.cut(\n",
    "        df['depression_score'],\n",
    "        bins=[-np.inf, q1, q3, np.inf],\n",
    "        labels=['low', 'medium', 'high']\n",
    "    )\n",
    "    \n",
    "    # Create binary depression (1 for high, 0 for others)\n",
    "    df['depression_binary'] = (df['depression_category'] == 'high').astype(int)\n",
    "    \n",
    "    # Print distribution information\n",
    "    print(\"\\nDepression Score Statistics:\")\n",
    "    print(f\"Mean: {mean:.2f}\")\n",
    "    print(f\"Std: {std:.2f}\")\n",
    "    print(f\"Q1 (25th percentile): {q1:.2f}\")\n",
    "    print(f\"Q3 (75th percentile): {q3:.2f}\")\n",
    "    print(f\"IQR: {iqr:.2f}\")\n",
    "    \n",
    "    print(\"\\nThresholds:\")\n",
    "    print(f\"Z-score method (2 SD above mean): {z_score_threshold:.2f}\")\n",
    "    print(f\"IQR method (Q3 + 1.5*IQR): {iqr_threshold:.2f}\")\n",
    "    \n",
    "    print(\"\\nDepression Categories Distribution:\")\n",
    "    print(df['depression_category'].value_counts(normalize=True))\n",
    "    \n",
    "    return df\n",
    "\n",
    "class DataPreprocessor:\n",
    "    \"\"\"\n",
    "    A class to handle all data preprocessing steps for the ABCD dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, missing_threshold=0.2, \n",
    "                 cont_var_threshold=0.01, \n",
    "                 cat_var_threshold=0.01,\n",
    "                 correlation_threshold=0.95,\n",
    "                 unique_threshold=20,\n",
    "                 sample_ratio_threshold=0.1,\n",
    "                 output_dir='../data/processed'):\n",
    "        \"\"\"\n",
    "        Initialize preprocessing parameters\n",
    "        \"\"\"\n",
    "        self.missing_threshold = missing_threshold\n",
    "        self.cont_var_threshold = cont_var_threshold\n",
    "        self.cat_var_threshold = cat_var_threshold\n",
    "        self.correlation_threshold = correlation_threshold\n",
    "        self.unique_threshold = unique_threshold\n",
    "        self.sample_ratio_threshold = sample_ratio_threshold\n",
    "        self.output_dir = output_dir\n",
    "        self.encoders = {}\n",
    "        self.scaler = None\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    def identify_variable_types(self, df):\n",
    "        \"\"\"\n",
    "        Identify continuous, categorical, and binary variables\n",
    "        \"\"\"\n",
    "        continuous_vars = []\n",
    "        categorical_vars = []\n",
    "        binary_vars = []\n",
    "        \n",
    "        for col in df.columns:\n",
    "            if col in ['src_subject_id', 'depression_score']:\n",
    "                continue\n",
    "                \n",
    "            n_unique = df[col].nunique()\n",
    "            n_samples = len(df)\n",
    "            unique_ratio = n_unique / n_samples\n",
    "            is_numeric = pd.api.types.is_numeric_dtype(df[col])\n",
    "            \n",
    "            # Binary variables\n",
    "            if n_unique == 2:\n",
    "                binary_vars.append(col)\n",
    "                continue\n",
    "                \n",
    "            # Non-numeric variables are categorical\n",
    "            if not is_numeric:\n",
    "                categorical_vars.append(col)\n",
    "                continue\n",
    "                \n",
    "            # Numeric variables need further investigation\n",
    "            if is_numeric:\n",
    "                is_integer = df[col].dtype in ['int32', 'int64']\n",
    "                if (is_integer and n_unique <= self.unique_threshold) or \\\n",
    "                   (n_unique <= self.unique_threshold and unique_ratio <= self.sample_ratio_threshold):\n",
    "                    value_counts = df[col].value_counts(normalize=True)\n",
    "                    if value_counts.max() < 0.5:  # No single value dominates\n",
    "                        categorical_vars.append(col)\n",
    "                    else:\n",
    "                        continuous_vars.append(col)\n",
    "                else:\n",
    "                    continuous_vars.append(col)\n",
    "        \n",
    "        return continuous_vars, categorical_vars, binary_vars\n",
    "    \n",
    "    def handle_special_values(self, df):\n",
    "        \"\"\"\n",
    "        Convert special values (555, 777, 999) to NaN before missing value analysis\n",
    "        \"\"\"\n",
    "        print(\"\\nHandling special values...\")\n",
    "        special_values = [555, 777, 999]\n",
    "        special_value_counts = {val: 0 for val in special_values}\n",
    "        \n",
    "        # Count special values in each column\n",
    "        for col in df.columns:\n",
    "            if col in ['src_subject_id', 'depression_score']:\n",
    "                continue\n",
    "            for val in special_values:\n",
    "                count = (df[col] == val).sum()\n",
    "                if count > 0:\n",
    "                    special_value_counts[val] += count\n",
    "                    print(f\"Found {count} instances of {val} in {col}\")\n",
    "                    # Convert to NaN\n",
    "                    df[col] = df[col].replace(val, np.nan)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\nSpecial value summary:\")\n",
    "        for val, count in special_value_counts.items():\n",
    "            if count > 0:\n",
    "                print(f\"Total {val} values converted to NaN: {count}\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def handle_missing_values(self, df, depression_df):\n",
    "        \"\"\"\n",
    "        Handle missing values in the dataset:\n",
    "        1. Convert special values to NaN\n",
    "        2. Remove variables with high missingness\n",
    "        3. Impute missing values:\n",
    "           - Mean imputation for numerical variables\n",
    "           - Mode imputation for categorical variables\n",
    "        \"\"\"\n",
    "        print(\"\\nHandling missing values...\")\n",
    "        \n",
    "        # Step 1: Convert special values to NaN\n",
    "        df = self.handle_special_values(df)\n",
    "        \n",
    "        # Step 2: Calculate missingness after special value conversion\n",
    "        merged_df = pd.merge(df, depression_df[['src_subject_id']], \n",
    "                            on='src_subject_id', how='inner')\n",
    "        missingness = merged_df.isnull().sum() / len(merged_df)\n",
    "        \n",
    "        # Print missing value statistics before removal\n",
    "        print(\"\\nMissing value statistics before removing high missingness variables:\")\n",
    "        high_missing_vars = missingness[missingness > self.missing_threshold]\n",
    "        if not high_missing_vars.empty:\n",
    "            print(\"\\nVariables with high missingness:\")\n",
    "            for var, missing_pct in high_missing_vars.items():\n",
    "                print(f\"  - {var}: {missing_pct:.1%} missing\")\n",
    "        \n",
    "        # Remove columns with high missingness\n",
    "        keep_cols = missingness[missingness <= self.missing_threshold].index\n",
    "        clean_df = df[keep_cols].copy()\n",
    "        print(f\"\\nRemoved {len(df.columns) - len(keep_cols)} variables due to missingness\")\n",
    "        \n",
    "        # Identify numeric and categorical columns for imputation\n",
    "        numeric_cols = clean_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "        numeric_cols = [col for col in numeric_cols if col not in ['src_subject_id', 'depression_score']]\n",
    "        \n",
    "        categorical_cols = clean_df.select_dtypes(exclude=['int64', 'float64']).columns\n",
    "        categorical_cols = [col for col in categorical_cols if col not in ['src_subject_id', 'depression_score']]\n",
    "        \n",
    "        # Calculate and print missing value statistics before imputation\n",
    "        print(\"\\nMissing values before imputation:\")\n",
    "        \n",
    "        # Numeric variables\n",
    "        numeric_missing = clean_df[numeric_cols].isnull().sum()\n",
    "        total_numeric_missing = numeric_missing.sum()\n",
    "        if total_numeric_missing > 0:\n",
    "            print(\"\\nNumeric variables:\")\n",
    "            print(f\"Total missing values: {total_numeric_missing}\")\n",
    "            for col in numeric_cols:\n",
    "                n_missing = numeric_missing[col]\n",
    "                if n_missing > 0:\n",
    "                    print(f\"  - {col}: {n_missing} missing values ({n_missing/len(clean_df):.1%})\")\n",
    "        \n",
    "        # Categorical variables\n",
    "        categorical_missing = clean_df[categorical_cols].isnull().sum()\n",
    "        total_categorical_missing = categorical_missing.sum()\n",
    "        if total_categorical_missing > 0:\n",
    "            print(\"\\nCategorical variables:\")\n",
    "            print(f\"Total missing values: {total_categorical_missing}\")\n",
    "            for col in categorical_cols:\n",
    "                n_missing = categorical_missing[col]\n",
    "                if n_missing > 0:\n",
    "                    print(f\"  - {col}: {n_missing} missing values ({n_missing/len(clean_df):.1%})\")\n",
    "        \n",
    "        # Mean imputation for continuuos variables\n",
    "        print(\"\\nPerforming mean imputation for numeric variables...\")\n",
    "        for col in numeric_cols:\n",
    "            if clean_df[col].isnull().any():\n",
    "                mean_value = clean_df[col].mean()\n",
    "                n_missing = clean_df[col].isnull().sum()\n",
    "                clean_df[col] = clean_df[col].fillna(mean_value)\n",
    "                print(f\"Imputed {n_missing} missing values in {col} with mean: {mean_value:.2f}\")\n",
    "        \n",
    "        #  Mode imputation for categorical variables\n",
    "        print(\"\\nPerforming mode imputation for categorical variables...\")\n",
    "        for col in categorical_cols:\n",
    "            if clean_df[col].isnull().any():\n",
    "                mode_value = clean_df[col].mode()[0]  # Get the most common value\n",
    "                n_missing = clean_df[col].isnull().sum()\n",
    "                clean_df[col] = clean_df[col].fillna(mode_value)\n",
    "                print(f\"Imputed {n_missing} missing values in {col} with mode: {mode_value}\")\n",
    "        \n",
    "        # Verify no missing values \n",
    "        remaining_numeric_missing = clean_df[numeric_cols].isnull().sum().sum()\n",
    "        remaining_categorical_missing = clean_df[categorical_cols].isnull().sum().sum()\n",
    "        \n",
    "        if remaining_numeric_missing > 0 or remaining_categorical_missing > 0:\n",
    "            print(f\"\\nWarning: {remaining_numeric_missing + remaining_categorical_missing} missing values remain after imputation\")\n",
    "            if remaining_numeric_missing > 0:\n",
    "                print(f\"  - {remaining_numeric_missing} in numeric columns\")\n",
    "            if remaining_categorical_missing > 0:\n",
    "                print(f\"  - {remaining_categorical_missing} in categorical columns\")\n",
    "        else:\n",
    "            print(\"\\nAll missing values have been imputed\")\n",
    "        \n",
    "        return clean_df\n",
    "    \n",
    "    def remove_low_variance_variables(self, df, continuous_vars):\n",
    "        \"\"\"\n",
    "        Remove variables with low variance\n",
    "        \"\"\"\n",
    "        print(\"\\nRemoving low variance variables...\")\n",
    "        n_cols_before = len(df.columns)\n",
    "        removed_vars = []\n",
    "        \n",
    "        for var in continuous_vars:\n",
    "            if var in ['src_subject_id', 'depression_score']:\n",
    "                continue\n",
    "            if var not in df.columns:\n",
    "                continue\n",
    "            std = df[var].std()\n",
    "            mean = df[var].mean()\n",
    "            if mean != 0:\n",
    "                cv = std / abs(mean)\n",
    "                if cv <= self.cont_var_threshold:\n",
    "                    df = df.drop(columns=[var])\n",
    "                    removed_vars.append(f\"{var} (continuous, cv={cv:.3f})\")\n",
    "        \n",
    "        print(f\"Removed {n_cols_before - len(df.columns)} low variance variables\")\n",
    "        if removed_vars:\n",
    "            print(\"\\nRemoved variables:\")\n",
    "            for var in removed_vars:\n",
    "                print(f\"  - {var}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    def track_variables(self, df, stage_name):\n",
    "        \"\"\"\n",
    "        Track which variables are present at each stage\n",
    "        \"\"\"\n",
    "        print(f\"\\nVariables present after {stage_name}:\")\n",
    "        print(f\"Total columns: {len(df.columns)}\")\n",
    "        numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "        print(f\"Numeric columns: {len(numeric_cols)}\")\n",
    "        categorical_cols = df.select_dtypes(exclude=['int64', 'float64']).columns\n",
    "        print(f\"Categorical columns: {len(categorical_cols)}\")\n",
    "        return df\n",
    "\n",
    "    def encode_categorical_variables(self, df, categorical_vars, binary_vars):\n",
    "        \"\"\"\n",
    "        Encode categorical and binary variables\n",
    "        \"\"\"\n",
    "        print(\"\\nEncoding categorical variables...\")\n",
    "        \n",
    "        # binary variables \n",
    "        for var in binary_vars:\n",
    "            if var in df.columns:\n",
    "                print(f\"Using binary encoding for {var}\")\n",
    "                df[var] = (df[var] == df[var].value_counts().index[0]).astype(int)\n",
    "                self.encoders[var] = 'binary'\n",
    "        \n",
    "        #  categorical variables\n",
    "        for var in categorical_vars:\n",
    "            if var in df.columns:\n",
    "                if df[var].dtype in ['int64', 'float64'] or len(df[var].unique()) > 10:\n",
    "                    print(f\"Using ordinal encoding for {var}\")\n",
    "                    self.encoders[var] = OrdinalEncoder(handle_unknown='use_encoded_value', \n",
    "                                                      unknown_value=-1)\n",
    "                    values = df[var].values.reshape(-1, 1)\n",
    "                    df[var] = self.encoders[var].fit_transform(values)\n",
    "                else:\n",
    "                    print(f\"Using one-hot encoding for {var}\")\n",
    "                    self.encoders[var] = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "                    values = df[var].values.reshape(-1, 1)\n",
    "                    encoded_values = self.encoders[var].fit_transform(values)\n",
    "                    feature_names = [f\"{var}_{i}\" for i in range(encoded_values.shape[1])]\n",
    "                    encoded_df = pd.DataFrame(encoded_values, columns=feature_names, index=df.index)\n",
    "                    df = pd.concat([df.drop(columns=[var]), encoded_df], axis=1)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def scale_numerical_variables(self, df, continuous_vars):\n",
    "            \"\"\"\n",
    "            Scale numerical variables including depression score\n",
    "            \"\"\"\n",
    "            print(\"\\nScaling numerical variables...\")\n",
    "            self.scaler = StandardScaler()\n",
    "            \n",
    "            # Filter continuous_vars to only include those that still exist in the dataframe\n",
    "            existing_continuous_vars = [var for var in continuous_vars if var in df.columns]\n",
    "            \n",
    "            # Add depression score to variables to scale\n",
    "            vars_to_scale = existing_continuous_vars + ['depression_score']\n",
    "            \n",
    "            if not vars_to_scale:\n",
    "                print(\"No continuous variables found to scale\")\n",
    "                return df\n",
    "                \n",
    "            print(f\"Scaling {len(vars_to_scale)} continuous variables (including depression score)\")\n",
    "            df[vars_to_scale] = self.scaler.fit_transform(df[vars_to_scale])\n",
    "            \n",
    "            # Print statistics before and after scaling\n",
    "            print(\"\\nDepression score statistics before scaling:\")\n",
    "            print(df['depression_score'].describe())\n",
    "            \n",
    "            return df\n",
    "\n",
    "    def transform_skewed_features(self, df, continuous_vars):\n",
    "        \"\"\"\n",
    "        Apply log transformation to skewed features\n",
    "        \"\"\"\n",
    "        print(\"\\nApplying log transformation to skewed features...\")\n",
    "        \n",
    "        # Filter continuous_vars to only include those that still exist in the dataframe\n",
    "        existing_continuous_vars = [var for var in continuous_vars if var in df.columns]\n",
    "        \n",
    "        if not existing_continuous_vars:\n",
    "            print(\"No continuous variables found to transform\")\n",
    "            return df\n",
    "            \n",
    "        for col in existing_continuous_vars:\n",
    "            skewness = stats.skew(df[col].dropna())\n",
    "            if abs(skewness) > 1:\n",
    "                print(f\"Log transforming {col} (skewness: {skewness:.2f})\")\n",
    "                df[col] = np.log1p(df[col] - df[col].min() + 1)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def remove_correlated_features(self, df):\n",
    "        \"\"\"\n",
    "        Remove highly correlated features\n",
    "        \"\"\"\n",
    "        print(\"\\nRemoving correlated features...\")\n",
    "        numeric_df = df.select_dtypes(include=['int64', 'float64'])\n",
    "        corr_matrix = numeric_df.corr().abs()\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        to_drop = [column for column in upper.columns if any(upper[column] > self.correlation_threshold)]\n",
    "        df = df.drop(columns=to_drop)\n",
    "        print(f\"Removed {len(to_drop)} highly correlated variables\")\n",
    "        return df\n",
    "\n",
    "        \n",
    "        return clean_df\n",
    "    \n",
    "    def remove_low_variance_variables(self, df, continuous_vars):\n",
    "        \"\"\"\n",
    "        Remove variables with low variance\n",
    "        \"\"\"\n",
    "        print(\"\\nRemoving low variance variables...\")\n",
    "        n_cols_before = len(df.columns)\n",
    "        removed_vars = []\n",
    "        \n",
    "        # Remove low variance continuous variables\n",
    "        for var in continuous_vars:\n",
    "            if var in ['src_subject_id', 'depression_score']:\n",
    "                continue\n",
    "            if var not in df.columns:\n",
    "                continue\n",
    "            std = df[var].std()\n",
    "            mean = df[var].mean()\n",
    "            if mean != 0:\n",
    "                cv = std / abs(mean)\n",
    "                if cv <= self.cont_var_threshold:\n",
    "                    df = df.drop(columns=[var])\n",
    "                    removed_vars.append(f\"{var} (continuous, cv={cv:.3f})\")\n",
    "        \n",
    "        print(f\"Removed {n_cols_before - len(df.columns)} low variance variables\")\n",
    "        if removed_vars:\n",
    "            print(\"\\nRemoved variables:\")\n",
    "            for var in removed_vars:\n",
    "                print(f\"  - {var}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    def track_variables(self, df, stage_name):\n",
    "        \"\"\"\n",
    "        Track which variables are present at each stage\n",
    "        \"\"\"\n",
    "        print(f\"\\nVariables present after {stage_name}:\")\n",
    "        print(f\"Total columns: {len(df.columns)}\")\n",
    "        numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "        print(f\"Numeric columns: {len(numeric_cols)}\")\n",
    "        categorical_cols = df.select_dtypes(exclude=['int64', 'float64']).columns\n",
    "        print(f\"Categorical columns: {len(categorical_cols)}\")\n",
    "        return df\n",
    "\n",
    "    def encode_categorical_variables(self, df, categorical_vars, binary_vars):\n",
    "        \"\"\"\n",
    "        Encode categorical and binary variables\n",
    "        \"\"\"\n",
    "        print(\"\\nEncoding categorical variables...\")\n",
    "        \n",
    "        # binary variables \n",
    "        for var in binary_vars:\n",
    "            if var in df.columns:\n",
    "                print(f\"Using binary encoding for {var}\")\n",
    "                df[var] = (df[var] == df[var].value_counts().index[0]).astype(int)\n",
    "                self.encoders[var] = 'binary'\n",
    "        \n",
    "        #  categorical variables\n",
    "        for var in categorical_vars:\n",
    "            if var in df.columns:\n",
    "                if df[var].dtype in ['int64', 'float64'] or len(df[var].unique()) > 10:\n",
    "                    print(f\"Using ordinal encoding for {var}\")\n",
    "                    self.encoders[var] = OrdinalEncoder(handle_unknown='use_encoded_value', \n",
    "                                                      unknown_value=-1)\n",
    "                    values = df[var].values.reshape(-1, 1)\n",
    "                    df[var] = self.encoders[var].fit_transform(values)\n",
    "                else:\n",
    "                    print(f\"Using one-hot encoding for {var}\")\n",
    "                    self.encoders[var] = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "                    values = df[var].values.reshape(-1, 1)\n",
    "                    encoded_values = self.encoders[var].fit_transform(values)\n",
    "                    feature_names = [f\"{var}_{i}\" for i in range(encoded_values.shape[1])]\n",
    "                    encoded_df = pd.DataFrame(encoded_values, columns=feature_names, index=df.index)\n",
    "                    df = pd.concat([df.drop(columns=[var]), encoded_df], axis=1)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def scale_numerical_variables(self, df, continuous_vars):\n",
    "        \"\"\"\n",
    "        Scale numerical variables including depression score\n",
    "        \"\"\"\n",
    "        print(\"\\nScaling numerical variables...\")\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        # Filter continuous_vars to only include those that still exist in the dataframe\n",
    "        existing_continuous_vars = [var for var in continuous_vars if var in df.columns]\n",
    "        \n",
    "        # Add depression score to variables to scale\n",
    "        vars_to_scale = existing_continuous_vars + ['depression_score']\n",
    "        \n",
    "        if not vars_to_scale:\n",
    "            print(\"No continuous variables found to scale\")\n",
    "            return df\n",
    "            \n",
    "        print(f\"Scaling {len(vars_to_scale)} continuous variables (including depression score)\")\n",
    "        df[vars_to_scale] = self.scaler.fit_transform(df[vars_to_scale])\n",
    "        \n",
    "        # Print statistics before and after scaling\n",
    "        print(\"\\nDepression score statistics before scaling:\")\n",
    "        print(df['depression_score'].describe())\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def transform_skewed_features(self, df, continuous_vars):\n",
    "        \"\"\"\n",
    "        Apply log transformation to skewed features\n",
    "        \"\"\"\n",
    "        print(\"\\nApplying log transformation to skewed features...\")\n",
    "        \n",
    "        # Filter continuous_vars to only include those that still exist in the dataframe\n",
    "        existing_continuous_vars = [var for var in continuous_vars if var in df.columns]\n",
    "        \n",
    "        if not existing_continuous_vars:\n",
    "            print(\"No continuous variables found to transform\")\n",
    "            return df\n",
    "            \n",
    "        for col in existing_continuous_vars:\n",
    "            skewness = stats.skew(df[col].dropna())\n",
    "            if abs(skewness) > 1:\n",
    "                print(f\"Log transforming {col} (skewness: {skewness:.2f})\")\n",
    "                df[col] = np.log1p(df[col] - df[col].min() + 1)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def remove_correlated_features(self, df):\n",
    "        \"\"\"\n",
    "        Remove highly correlated features\n",
    "        \"\"\"\n",
    "        print(\"\\nRemoving correlated features...\")\n",
    "        numeric_df = df.select_dtypes(include=['int64', 'float64'])\n",
    "        corr_matrix = numeric_df.corr().abs()\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        to_drop = [column for column in upper.columns if any(upper[column] > self.correlation_threshold)]\n",
    "        df = df.drop(columns=to_drop)\n",
    "        print(f\"Removed {len(to_drop)} highly correlated variables\")\n",
    "        return df\n",
    "\n",
    "    def preprocess(self, baseline_df, depression_df):\n",
    "        \"\"\"\n",
    "        Run the complete preprocessing pipeline without train/test split\n",
    "        \"\"\"\n",
    "        print(\"Starting data preprocessing pipeline...\")\n",
    "        \n",
    "        # Step 1: Handle missing values\n",
    "        df_before = baseline_df.copy()\n",
    "        df = self.handle_missing_values(baseline_df, depression_df)\n",
    "        \n",
    "        # Identify columns for visualization\n",
    "        numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "        numeric_cols = [col for col in numeric_cols if col not in ['src_subject_id', 'depression_score']]\n",
    "        \n",
    "        categorical_cols = df.select_dtypes(exclude=['int64', 'float64']).columns\n",
    "        categorical_cols = [col for col in categorical_cols if col not in ['src_subject_id', 'depression_score']]\n",
    "        \n",
    "        # Step 2: Identify variable types\n",
    "        continuous_vars, categorical_vars, binary_vars = self.identify_variable_types(df)\n",
    "        print(f\"\\nIdentified {len(continuous_vars)} continuous, {len(categorical_vars)} categorical, \"\n",
    "              f\"and {len(binary_vars)} binary variables\")\n",
    "        \n",
    "        # Step 3: Remove low variance variables\n",
    "        df = self.remove_low_variance_variables(df, continuous_vars)\n",
    "        df = self.track_variables(df, \"low variance removal\")\n",
    "                \n",
    "        # Step 4: Encode categorical variables\n",
    "        df = self.encode_categorical_variables(df, categorical_vars, binary_vars)\n",
    "        \n",
    "        # Step 5: Scale numerical variables\n",
    "        df = self.scale_numerical_variables(df, continuous_vars)\n",
    "        \n",
    "        # Step 6: Transform skewed features\n",
    "        df = self.transform_skewed_features(df, continuous_vars)\n",
    "        \n",
    "        # Step 7: Remove correlated features\n",
    "        df = self.remove_correlated_features(df)\n",
    "        \n",
    "        # Step 8: Add depression scores\n",
    "        df = pd.merge(df, depression_df[['src_subject_id', 'depression_score']], \n",
    "                     on='src_subject_id', how='inner')\n",
    "        \n",
    "        # Print final dataset information\n",
    "        print(\"\\nFinal preprocessed dataset:\")\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        print(\"\\nFeature types:\")\n",
    "        print(df.dtypes.value_counts())\n",
    "        \n",
    "        # Save the preprocessed dataset\n",
    "        output_path = Path(self.output_dir) / 'preprocessed_data.csv'\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"\\nPreprocessed data saved to: {output_path}\")\n",
    "        \n",
    "        # Return the preprocessed data and preprocessing objects\n",
    "        return {\n",
    "            'data': df,\n",
    "            'encoders': self.encoders,\n",
    "            'scaler': self.scaler,\n",
    "            'variable_types': {\n",
    "                'continuous': continuous_vars,\n",
    "                'categorical': categorical_vars,\n",
    "                'binary': binary_vars\n",
    "            }\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    # Load CBCL depression data\n",
    "    print(\"Loading CBCL depression data...\")\n",
    "    depression_df = load_cbcl_depression()\n",
    "    \n",
    "    # Create depression categories\n",
    "    print(\"\\nCreating depression categories...\")\n",
    "    depression_df = create_depression_categories(depression_df)\n",
    "    \n",
    "    # Load baseline data for our subjects\n",
    "    print(\"\\nStarting baseline data loading...\")\n",
    "    baseline_df = load_baseline_data(depression_df['src_subject_id'])\n",
    "    print(f\"\\nFinal baseline dataset shape: {baseline_df.shape}\")\n",
    "    \n",
    "    # Initialize preprocessor and run preprocessing pipeline\n",
    "    print(\"\\nStarting preprocessing pipeline...\")\n",
    "    preprocessor = DataPreprocessor()\n",
    "    preprocessed_data = preprocessor.preprocess(baseline_df, depression_df)\n",
    "    \n",
    "    print(\"\\nDone!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
