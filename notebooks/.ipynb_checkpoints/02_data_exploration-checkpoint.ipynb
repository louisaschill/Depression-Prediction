{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cb3b1b-7d88-4733-ad61-184d7c207d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02_data_exploration.ipynb\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import scipy.stats as stats\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_theme()\n",
    "\n",
    "# Load preprocessed data\n",
    "def load_preprocessed_data():\n",
    "    \"\"\"Load the preprocessed data and return the dataframe\"\"\"\n",
    "    data_path = Path('../data/processed/preprocessed_data.csv')\n",
    "    if not data_path.exists():\n",
    "        raise FileNotFoundError(\"Preprocessed data not found. Please run 01_data_preprocessing first.\")\n",
    "    \n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"Loaded data with shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "# Correlation analysis with depression scores\n",
    "def explore_correlations(df, dep_col='depression_score', corr_threshold=0.3):\n",
    "    \"\"\"Explore correlations with depression scores\"\"\"\n",
    "    print(\"\\nAnalyzing correlations with depression scores...\")\n",
    "    \n",
    "    # Calculate correlations\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    numeric_cols = [col for col in numeric_cols if col not in ['src_subject_id', 'depression_score']]\n",
    "    \n",
    "    correlations = df[numeric_cols].corrwith(df[dep_col]).sort_values(ascending=False)\n",
    "    \n",
    "    # Find highly correlated variables\n",
    "    high_corr = correlations[abs(correlations) >= corr_threshold]\n",
    "    \n",
    "    print(f\"\\nFound {len(high_corr)} variables with |correlation| >= {corr_threshold}\")\n",
    "    print(\"\\nTop 20 positive correlations:\")\n",
    "    print(high_corr.head(20))\n",
    "    print(\"\\nTop 20 negative correlations:\")\n",
    "    print(high_corr.tail(20))\n",
    "    \n",
    "    # Plot top correlations\n",
    "    top_n = 20\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    high_corr.head(top_n).plot(kind='bar')\n",
    "    plt.title(f'Top {top_n} Positive Correlations with Depression Score')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    high_corr.tail(top_n).plot(kind='bar')\n",
    "    plt.title(f'Top {top_n} Negative Correlations with Depression Score')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return high_corr\n",
    "\n",
    "# Depression score analysis\n",
    "def analyze_depression_scores(df):\n",
    "    \"\"\"Analyze the distribution and characteristics of depression scores\"\"\"\n",
    "    print(\"\\nDepression Score Analysis:\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"\\nBasic Statistics:\")\n",
    "    print(df['depression_score'].describe())\n",
    "    \n",
    "    # Distribution plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data=df, x='depression_score', bins=30)\n",
    "    plt.title('Distribution of Depression Scores')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate percentiles\n",
    "    percentiles = [25, 50, 75, 90, 95, 99]\n",
    "    print(\"\\nPercentiles:\")\n",
    "    for p in percentiles:\n",
    "        value = df['depression_score'].quantile(p/100)\n",
    "        print(f\"{p}th percentile: {value:.2f}\")\n",
    "\n",
    "# Feature importance analysis\n",
    "def analyze_feature_importance(df, top_n=100):\n",
    "    \"\"\"Analyze feature importance using Random Forest\"\"\"\n",
    "    print(\"\\nFeature Importance Analysis:\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X = df.drop(columns=['src_subject_id', 'depression_score'])\n",
    "    y = df['depression_score']\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Train Random Forest\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_scaled, y)\n",
    "    \n",
    "    # Get feature importance\n",
    "    importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': rf.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Print top 100 features and their importance scores\n",
    "    print(\"\\nTop 100 Most Important Features:\")\n",
    "    print(importance.head(100).to_string())\n",
    "    \n",
    "    # Plot top 100 features in multiple subplots for better readability\n",
    "    n_plots = 4  # Number of subplots\n",
    "    features_per_plot = top_n // n_plots\n",
    "    \n",
    "    plt.figure(figsize=(20, 5*n_plots))\n",
    "    for i in range(n_plots):\n",
    "        start_idx = i * features_per_plot\n",
    "        end_idx = (i + 1) * features_per_plot\n",
    "        \n",
    "        plt.subplot(n_plots, 1, i+1)\n",
    "        plot_data = importance.iloc[start_idx:end_idx]\n",
    "        sns.barplot(data=plot_data, x='importance', y='feature')\n",
    "        plt.title(f'Features {start_idx+1}-{end_idx} by Importance')\n",
    "        plt.tight_layout()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save importance scores to CSV\n",
    "    output_path = Path('../data/processed/feature_importance.csv')\n",
    "    importance.to_csv(output_path, index=False)\n",
    "    print(f\"\\nFeature importance scores saved to: {output_path}\")\n",
    "    \n",
    "    return importance\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the data\n",
    "    print(\"Loading preprocessed data...\")\n",
    "    df = load_preprocessed_data()\n",
    "    \n",
    "    # Analyze depression scores\n",
    "    print(\"\\n=== Depression Score Analysis ===\")\n",
    "    analyze_depression_scores(df)\n",
    "    \n",
    "    # Explore correlations\n",
    "    print(\"\\n=== Correlation Analysis ===\")\n",
    "    high_correlations = explore_correlations(df)\n",
    "    \n",
    "    # Analyze feature importance\n",
    "    print(\"\\n=== Feature Importance Analysis ===\")\n",
    "    feature_importance = analyze_feature_importance(df)\n",
    "    \n",
    "    print(\"\\nExploration complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803011dc-f05d-472d-a787-ca80cfa4ed83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
